{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics Generator using a Taylor Swift Lyrics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    LSTM,\n",
    "    Activation,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    TimeDistributed,\n",
    ")\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>track_title</th>\n",
       "      <th>track_n</th>\n",
       "      <th>lyric</th>\n",
       "      <th>line</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Tim McGraw</td>\n",
       "      <td>1</td>\n",
       "      <td>He said the way my blue eyes shined</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Tim McGraw</td>\n",
       "      <td>1</td>\n",
       "      <td>Put those Georgia stars to shame that night</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Tim McGraw</td>\n",
       "      <td>1</td>\n",
       "      <td>I said, \"That's a lie\"</td>\n",
       "      <td>3</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Tim McGraw</td>\n",
       "      <td>1</td>\n",
       "      <td>Just a boy in a Chevy truck</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Tim McGraw</td>\n",
       "      <td>1</td>\n",
       "      <td>That had a tendency of gettin' stuck</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         artist         album track_title  track_n  \\\n",
       "0  Taylor Swift  Taylor Swift  Tim McGraw        1   \n",
       "1  Taylor Swift  Taylor Swift  Tim McGraw        1   \n",
       "2  Taylor Swift  Taylor Swift  Tim McGraw        1   \n",
       "3  Taylor Swift  Taylor Swift  Tim McGraw        1   \n",
       "4  Taylor Swift  Taylor Swift  Tim McGraw        1   \n",
       "\n",
       "                                         lyric  line  year  \n",
       "0          He said the way my blue eyes shined     1  2006  \n",
       "1  Put those Georgia stars to shame that night     2  2006  \n",
       "2                       I said, \"That's a lie\"     3  2006  \n",
       "3                  Just a boy in a Chevy truck     4  2006  \n",
       "4         That had a tendency of gettin' stuck     5  2006  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Dataset\n",
    "dataset = pd.read_csv(\"data/taylor_swift_lyrics.csv\", encoding=\"latin1\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the lines of each song to get each song by its own string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_first_line(\n",
    "    lyrics: list, song_id: list, song_name: list, row: int\n",
    ") -> tuple[list, list, list]:\n",
    "    lyrics.append(row[\"lyric\"] + \"\\n\")\n",
    "    song_id.append(row[\"year\"] * 100 + row[\"track_n\"])\n",
    "    song_name.append(row[\"track_title\"])\n",
    "\n",
    "    return lyrics, song_id, song_name\n",
    "\n",
    "\n",
    "# define empty lists for the lyrics, song_id, song_name\n",
    "lyrics, song_id, song_name = [], [], []\n",
    "\n",
    "# song_number indicates the song number in the dataset\n",
    "song_number = 1\n",
    "\n",
    "# i indicates the song number\n",
    "i = 0\n",
    "is_first_line = True\n",
    "\n",
    "# Iterate through every lyrics line and join them together for each song independently\n",
    "for index, row in dataset.iterrows():\n",
    "    if song_number == row[\"track_n\"]:\n",
    "        if is_first_line:\n",
    "            lyrics, song_id, song_name = process_first_line(\n",
    "                lyrics, song_id, song_name, row\n",
    "            )\n",
    "            is_first_line = False\n",
    "        else:\n",
    "            # if we still in the same song, keep joining the lyrics lines\n",
    "            lyrics[i] += row[\"lyric\"] + \"\\n\"\n",
    "    # When it's done joining a song's lyrics lines , go to the next song :\n",
    "    else:\n",
    "        lyrics, song_id, song_name = process_first_line(lyrics, song_id, song_name, row)\n",
    "        songNumber = row[\"track_n\"]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new pandas dataframe to save song_id, song_name, lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_data = pd.DataFrame(\n",
    "    {\"song_id\": song_id, \"song_name\": song_name, \"lyrics\": lyrics}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving lyrics to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/lyrics_text.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
    "    for list_item in lyrics:\n",
    "        file.write(\"%s\\n\" % list_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert lyrics to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_name = \"data/lyrics_text.txt\"\n",
    "\n",
    "raw_rext = open(text_file_name, encoding=\"UTF-8\").read()\n",
    "raw_text = raw_rext.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 2 dictionaries, one to convert chars to ints, the other to convert ints back to chars\n",
    "chars = sorted(list(set(raw_text)))\n",
    "\n",
    "int_chars = dict((i, c) for i, c in enumerate(chars))\n",
    "chars_int = dict((i, c) for c, i in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters :  178312\n",
      "Total Vocab :  60\n"
     ]
    }
   ],
   "source": [
    "# Get number of chars and vocab in our text\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "\n",
    "print(\"Total Characters : \", n_chars)  # number of all the characters in lyrics_text.txt\n",
    "print(\"Total Vocab : \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns :  178212\n"
     ]
    }
   ],
   "source": [
    "# process the dataset:\n",
    "seq_len = 100\n",
    "data_X = []\n",
    "data_Y = []\n",
    "\n",
    "for i in range(0, n_chars - seq_len, 1):\n",
    "    # Input Sequence(will be used as samples)\n",
    "    seq_in = raw_text[i : i + seq_len]\n",
    "\n",
    "    # Output Sequence(will be used as target)\n",
    "    seq_out = raw_text[i + seq_len]\n",
    "\n",
    "    # Store samples in data_X\n",
    "    data_X.append([chars_int[char] for char in seq_in])\n",
    "\n",
    "    # Store targets in data_y\n",
    "    data_Y.append(chars_int[seq_out])\n",
    "\n",
    "n_patterns = len(data_X)\n",
    "print(\"Total Patterns : \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the X to be suitable to go into LSTM RNN:\n",
    "X = np.reshape(data_X, (n_patterns, seq_len, 1))\n",
    "\n",
    "# Normalizing Input Data\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output targets\n",
    "Y = np_utils.to_categorical(data_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_layer_num = 4  # number of LSTM layers\n",
    "layer_size = [256, 256, 256, 256]  # number of nodes in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an Input Layer\n",
    "model.add(\n",
    "    LSTM(\n",
    "        layer_size[0], input_shape=(X.shape[1], X.shape[2]), return_sequences=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some hidden layers\n",
    "for i in range(1, LSTM_layer_num):\n",
    "    model.add(LSTM(layer_size[i], return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the Data\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(Y.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 100, 256)          264192    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100, 256)          525312    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100, 256)          525312    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100, 256)          525312    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25600)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 60)                1536060   \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 60)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,376,188\n",
      "Trainable params: 3,376,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Checkpoint:\n",
    "checkpoint_name = \"Weights-LSTM-improvement-{epoch:03d}-{loss:.5f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_name, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\"\n",
    ")\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  13/1114 [..............................] - ETA: 47:20 - loss: 3.3025"
     ]
    }
   ],
   "source": [
    "# Fit the model :\n",
    "model_params = {\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 128,\n",
    "    \"callbacks\": callbacks_list,\n",
    "    \"verbose\": 1,\n",
    "    \"validation_split\": 0.2,\n",
    "    \"validation_data\": None,\n",
    "    \"shuffle\": True,\n",
    "    \"initial_epoch\": 0,\n",
    "    \"steps_per_epoch\": None,\n",
    "    \"validation_steps\": None,\n",
    "}\n",
    "model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=model_params[\"epochs\"],\n",
    "    batch_size=model_params[\"batch_size\"],\n",
    "    callbacks=model_params[\"callbacks\"],\n",
    "    verbose=model_params[\"verbose\"],\n",
    "    validation_split=model_params[\"validation_split\"],\n",
    "    validation_data=model_params[\"validation_data\"],\n",
    "    shuffle=model_params[\"shuffle\"],\n",
    "    initial_epoch=model_params[\"initial_epoch\"],\n",
    "    steps_per_epoch=model_params[\"steps_per_epoch\"],\n",
    "    validation_steps=model_params[\"validation_steps\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6df78ae13f3cf31ed03fbc7e777cc7c43e76b3dc5757250f1c17b90e18282a6b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
